"""
æŠ–éŸ³çƒ­æ¦œæŠ“å–å¹¶æ¨é€åˆ°é£ä¹¦ç¾¤
ä¸»ç¨‹åºå…¥å£
"""
import os
import time
import logging
import schedule
from datetime import datetime
from dotenv import load_dotenv

from douyin_scraper import DouyinScraper
from feishu_notifier import FeishuNotifier
from config_loader import ConfigLoader


# é…ç½®æ—¥å¿—
def setup_logger():
    """é…ç½®æ—¥å¿—ç³»ç»Ÿ"""
    log_level = os.getenv('LOG_LEVEL', 'INFO')
    logging.basicConfig(
        level=getattr(logging, log_level),
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
        handlers=[
            logging.StreamHandler(),
            logging.FileHandler('douyin_hot_scraper.log', encoding='utf-8')
        ]
    )


logger = logging.getLogger(__name__)


def scrape_and_send(config_loader=None):
    """æŠ“å–çƒ­æ¦œå¹¶å‘é€åˆ°é£ä¹¦ç¾¤"""
    try:
        logger.info("=" * 50)
        logger.info("å¼€å§‹æ‰§è¡Œçƒ­æ¦œæŠ“å–ä»»åŠ¡")

        # è·å–ç¯å¢ƒå˜é‡
        webhook_url = os.getenv('FEISHU_WEBHOOK_URL')
        if not webhook_url:
            logger.error("æœªé…ç½® FEISHU_WEBHOOK_URLï¼Œè¯·æ£€æŸ¥ .env æ–‡ä»¶")
            return

        # ä½¿ç”¨ä¼ å…¥çš„é…ç½®æˆ–åˆ›å»ºæ–°çš„
        if not config_loader:
            config_loader = ConfigLoader()

        # è·å–é…ç½®
        scraper_config = config_loader.get_scraper_config()
        limit = scraper_config.get('limit', 20)

        # è·å–å¯ç”¨çš„æ¿å—
        enabled_categories = os.getenv('ENABLED_CATEGORIES', '').strip()
        category = enabled_categories.split(',')[0].strip() if enabled_categories else 'all'

        # åˆ›å»ºæŠ“å–å™¨å’Œé€šçŸ¥å™¨
        scraper = DouyinScraper(config_loader)
        notifier = FeishuNotifier(webhook_url)

        # æŠ“å–çƒ­æ¦œ
        hot_list = scraper.fetch_hot_list(limit=limit, category=category)

        if not hot_list:
            logger.error("æœªèƒ½è·å–çƒ­æ¦œæ•°æ®")
            # å‘é€é”™è¯¯é€šçŸ¥
            notifier.send_text_message("âš ï¸ æŠ–éŸ³çƒ­æ¦œæŠ“å–å¤±è´¥ï¼Œè¯·æ£€æŸ¥æœåŠ¡çŠ¶æ€")
            return

        # æ£€æŸ¥æ˜¯å¦ä½¿ç”¨äº†æµ‹è¯•æ•°æ®
        if scraper.is_using_test_data:
            logger.warning("âš ï¸  æ³¨æ„ï¼šå½“å‰ä½¿ç”¨çš„æ˜¯æµ‹è¯•æ•°æ®ï¼ŒæŠ–éŸ³ API å¯èƒ½æ— æ³•è®¿é—®")

        # å‘é€åˆ°é£ä¹¦
        # å¦‚æœä½¿ç”¨æµ‹è¯•æ•°æ®ï¼Œç›´æ¥å‘é€æ–‡æœ¬æ¶ˆæ¯
        if scraper.is_using_test_data:
            text_content = scraper.format_hot_list_text(hot_list, is_test_data=True, category=category)
            success = notifier.send_text_message(text_content)
        else:
            # ä¼˜å…ˆä½¿ç”¨äº¤äº’å¼å¡ç‰‡ï¼Œå¤±è´¥åˆ™ä½¿ç”¨æ–‡æœ¬æ¶ˆæ¯
            success = notifier.send_interactive_message(hot_list, source_name=scraper.current_source_name)

            if not success:
                logger.warning("äº¤äº’å¼å¡ç‰‡å‘é€å¤±è´¥ï¼Œå°è¯•ä½¿ç”¨æ–‡æœ¬æ¶ˆæ¯")
                text_content = scraper.format_hot_list_text(hot_list, is_test_data=False, category=category)
                success = notifier.send_text_message(text_content)

        if success:
            logger.info("çƒ­æ¦œæ•°æ®å‘é€æˆåŠŸ")
        else:
            logger.error("çƒ­æ¦œæ•°æ®å‘é€å¤±è´¥")

        logger.info("çƒ­æ¦œæŠ“å–ä»»åŠ¡å®Œæˆ")
        logger.info("=" * 50)

    except Exception as e:
        logger.error(f"æ‰§è¡Œä»»åŠ¡æ—¶å‘ç”Ÿé”™è¯¯: {e}", exc_info=True)


def main():
    """ä¸»å‡½æ•°"""
    # åŠ è½½ç¯å¢ƒå˜é‡
    load_dotenv()

    # é…ç½®æ—¥å¿—
    setup_logger()

    logger.info("ğŸš€ çƒ­æ¦œæŠ“å–æœåŠ¡å¯åŠ¨")

    # åŠ è½½é…ç½®
    config_loader = ConfigLoader()

    # è·å–é…ç½®
    interval_hours = int(os.getenv('SCRAPE_INTERVAL_HOURS', '1'))
    webhook_url = os.getenv('FEISHU_WEBHOOK_URL')

    # éªŒè¯é…ç½®
    if not webhook_url:
        logger.error("âŒ æœªé…ç½® FEISHU_WEBHOOK_URL")
        logger.error("è¯·å¤åˆ¶ .env.example ä¸º .env å¹¶å¡«å…¥ä½ çš„é£ä¹¦ Webhook URL")
        return

    # æ˜¾ç¤ºé…ç½®ä¿¡æ¯
    enabled_sources = config_loader.get_enabled_data_sources()
    enabled_categories = config_loader.get_enabled_categories()

    logger.info(f"âš™ï¸  é…ç½®ä¿¡æ¯:")
    logger.info(f"   - æŠ“å–é—´éš”: æ¯ {interval_hours} å°æ—¶")
    logger.info(f"   - å¯ç”¨æ•°æ®æº: {', '.join([s.get('name', k) for k, s in enabled_sources.items()])}")
    logger.info(f"   - å¯ç”¨æ¿å—: {', '.join([c.get('name', k) for k, c in enabled_categories.items()])}")
    logger.info(f"   - Webhook: {webhook_url[:50]}...")

    # ç«‹å³æ‰§è¡Œä¸€æ¬¡
    logger.info("ğŸ”„ ç«‹å³æ‰§è¡Œé¦–æ¬¡æŠ“å–...")
    scrape_and_send(config_loader)

    # è®¾ç½®å®šæ—¶ä»»åŠ¡
    logger.info(f"â° è®¾ç½®å®šæ—¶ä»»åŠ¡: æ¯ {interval_hours} å°æ—¶æ‰§è¡Œä¸€æ¬¡")
    schedule.every(interval_hours).hours.do(scrape_and_send, config_loader)

    # å¾ªç¯æ‰§è¡Œ
    logger.info("âœ… æœåŠ¡è¿è¡Œä¸­ï¼ŒæŒ‰ Ctrl+C é€€å‡º")

    try:
        while True:
            schedule.run_pending()
            time.sleep(60)  # æ¯åˆ†é’Ÿæ£€æŸ¥ä¸€æ¬¡å¾…æ‰§è¡Œçš„ä»»åŠ¡
    except KeyboardInterrupt:
        logger.info("\nğŸ‘‹ æœåŠ¡å·²åœæ­¢")


if __name__ == "__main__":
    main()
